{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen3 Butler Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "This tutorial is intended to run against a DESC DC2 data repository at NERSC, but it should run with only minor changes to collection names and data ID values in any Gen3 repo containing DRP pipeline outputs, and it was created by modifying a tutorial written for the ci_hsc_gen3 data repository, and has not been tested on its target data repository (yet).  This version of the notebook was written against stack release `w_2020_27`.  Some of the APIs used near the end are expected to change soon, but only slightly, and a new version of the notebook will be released then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "REPO_ROOT = \"/global/cscratch1/sd/desc/DC2/gen3/Run2.2i-gen3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Repositories and Collections\n",
    "\n",
    "As in Gen2, you initialize a `Butler` by pointing at a data repository, which is usually represented by a directory.\n",
    "\n",
    "That is _mostly_ true in Gen3 as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.daf.butler import Butler\n",
    "butler = Butler(REPO_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Gen3 repository roots will have a `butler.yaml` and possibly a `gen3.sqlite3` file.  A reasonable first thing to do when looking at a repository is to examine its _collections_, which are groups of datasets that frequently (but not always) correspond to processing runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(butler.registry.queryCollections())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we wrapped the result in `list(...)` - that's because most butler query operations return lazy iterators of some kind\n",
    "\n",
    "In fact, normally we'd construct a `Butler` instance with the name of one or more collections to read from as well as the repo root.  That's because a Gen3 data repository is actually more like a set of related Gen2 data repositories (_e.g._ `/datasets/hsc/repo` and all of the data repositories that (recursively) consider it a parent.  A Gen3 `collection` is like a specific Gen2 data repository, and while some collections may also be associated with subdirectories, that's not true in general.\n",
    "\n",
    "The `desc/demo` collection is the one that holds the results of building ci_hsc_gen3 (and hence running a bunch of `PipelineTask`s, so we'll reconstruct a butler with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler = Butler(REPO_ROOT, collections=\"desc/demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can pass either a single string or a sequence of strings as the collections argument.  In most places where a sequence of collections is expected, you can pass a large number of things.  See [Collection Expressions](https://pipelines.lsst.io/v/weekly/modules/lsst.daf.butler/queries.html#collection-expressions) for more information.  In fact, [`Registry.queryCollections`](https://pipelines.lsst.io/v/weekly/py-api/lsst.daf.butler.Registry.html#lsst.daf.butler.Registry.queryCollections) is one such place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "list(butler.registry.queryCollections(re.compile(\"LSST\\-ImSim\\/.*\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth pointing out the `collections` argument to the `Butler` constructor is used for direct `Butler` methods, but isn't used by any methods on the `butler.registry` attribute that we've been using so far.  The next section explains why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registry and Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last call was actually on `butler.registry`, not just `butler`, and that'll be a relatively common occurrence in Gen3, because a `Butler` is really just a convenience wrapper that combines three things:\n",
    "\n",
    "* a `Registry` instance that manages metadata and relationships between datasets via a SQL database (PostgreSQL, in this case);\n",
    "* a `Datastore` instance that manages the datasets themselves (files subdirectories of the repo root, in this case);\n",
    "* the name(s) of one or more collections.\n",
    "\n",
    "You'll frequently use `butler.registry` to perform operations that don't need anything from the `Datastore`.\n",
    "\n",
    "A `butler.datastore` attribute exists as well, but it's much less likely that you'll need to use it directly (I can't think of a reason).\n",
    "\n",
    "Neither the `Registry` nor the `Datastore` know about the `collections` you passed when constructing the `Butler`, so when using them directly you many need to pass `butler.collections` to them (if the method takes a `collections` argument)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to spell `get`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common thing you'll do with a `Butler` is call `get` (`PipelineTasks` call `put` just about as often, but usually the author of a concrete `PipelineTask` won't actually write any `get` and `put` calls).  In it's simplest form, that looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataId = {\"skymap\": \"imsim_skymap\", \"tract\": 3078, \"patch\": 26, \"abstract_filter\": \"z\"}\n",
    "coadd = butler.get(\"deepCoadd\", dataId=dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot to unpack here, but let's start by making it clear that you can write this a few different ways, and they're all equivalent (and most of them are identical to Gen2, aside from what's in the data ID):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass data ID as a positional argument:\n",
    "coadd = butler.get(\"deepCoadd\", dataId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass data ID as multiple keyword arguments:\n",
    "coadd = butler.get(\"deepCoadd\", skymap=\"imsim_skymap\", tract=3078, patch=26, abstract_filter=\"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do both.  Keyword arguments override the data ID dict (considered a feature, though it may be suprising).\n",
    "coadd = butler.get(\"deepCoadd\", dataId, patch=26, abstract_filter=\"z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetTypes\n",
    "\n",
    "The first argument can also be a `DatasetType` object instead of the string that refers to one.  A `DatasetType` instance knows the data ID keys needed to identify it (we call those \"dimensions\") and its StorageClass, which you can think of as a mapping to the Python type you'll get back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deepCoaddType = butler.registry.getDatasetType(\"deepCoadd\")\n",
    "print(deepCoaddType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Gen2 butler, all dataset types had to be pre-declared in an obs_ package.  In Gen3, they're added to the data repository as needed by the `PipelineTasks` that create them, so if you want to know what dataset types exist, you'll need to ask the registry (someday we'll hopefully find some way to put a snapshot of a \"typical\" registry's dataset types in the online documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(butler.registry.queryDatasetTypes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of those `DatasetTypes` have implicit component dataset types, like `deepCoadd.psf`.  You can `get` components just as you would their parents, and usually that'll be much more efficient if the component is a small piece of the whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = butler.get(\"deepCoadd.coaddInputs\", dataId)\n",
    "print(inputs.visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyhow, as promised, you can use that `DatasetType` instance in `get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coadd = butler.get(deepCoaddType, skymap=\"imsim_skymap\", tract=3078, patch=26, abstract_filter=\"z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetRefs\n",
    "\n",
    "The Gen3 Butler makes use of the combination of a `DatasetType` and a data ID frequently enough that there is a special object for that, `DatasetRef`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lsst.daf.butler import DatasetRef\n",
    "ref = DatasetRef(deepCoaddType, {\"skymap\": \"imsim_skymap\", \"tract\": 3078, \"patch\": 26, \"abstract_filter\": \"z\"})\n",
    "print(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass a `DatasetRef` as the _only_ argument to `get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coadd = butler.get(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're familiar with Gen2, you might have noticed that a Gen2 **DataRef** (which has a `Butler`) is _quite_ different from a Gen3 **DatasetRef** (which does not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component datasets are used to get predefined, differently-typed pieces of a composite dataset.  For some dataset types it's desirable to get same-typed, parameterized subsets, and that's what the `parameters` keyword argument to `get` is for.  The classic case is a subimage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.geom import Box2I, Point2I\n",
    "# This is probably not a reasonable box for this data ID; would need to\n",
    "# look at coadd.getBBox() from the previous step to guess one.\n",
    "bbox = Box2I(Point2I(20000, 16000), Point2I(20200, 18000))\n",
    "parameters = {\"bbox\": bbox}\n",
    "subcoadd = butler.get(\"deepCoadd\", dataId, parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "assert np.all(subcoadd.image.array == coadd[bbox].image.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Gen2,\n",
    "\n",
    "- The dataset type name is the same (still just `deepCoadd`, not `deepCoadd_sub`, as it was in Gen2).\n",
    "\n",
    "- You pass all parameters as a single dict as the `parameters` kwarg, rather than as separate kwargs that could get confused with the data ID.]\n",
    "\n",
    "Any of the alternate spellings of `get` shown above can be used with parameters, including the `DatasetRef` one - the parameters go in the call to `get`, not inside the `DatasetRef`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcoadd = butler.get(ref, parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying for Datasets\n",
    "\n",
    "One of the most important new features of the Gen3 Butler is much more complete support for querying datasets.  That typically goes through the `queryDatasets` method.  A typical query might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(butler.registry.queryDatasets(\"deepCoadd\", collections=[\"desc/demo\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we've wrapped the call in `list` because `queryDatasets` returns a single-pass iterator, not a container.  **It's also not guaranteed to return unique results**, because it might be much more expensive to do some kinds of deduplication (especially for complex queries) in the database.  We'll have a way to explicitly ask for database-side deduplication (basically `SELECT DISTICT`) very soon, but you can also just put the results in a `set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass a single data ID, either as a single argument or (as with `get`) a number of keyword arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(butler.registry.queryDatasets(\"deepCoadd\", collections=[\"desc/demo\"],\n",
    "                                   dataId={\"abstract_filter\": \"z\"}, deduplicate=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That data ID doesn't even have to be directly related to the dataset; `queryDatasets` will automatically use temporal or spatial overlaps if it needs to.  Here's a query for all of the calexps that overlap a patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(butler.registry.queryDatasets(\"calexp\", collections=[\"desc/demo\"],\n",
    "                                   skymap=\"imsim_skymap\", tract=3078, patch=26))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a big caveat to these spatial lookups, though: while they're guaranteed to return any dataset that overlaps the given data ID, they may also return some that don't, because the regions for observations are defined during ingest, and we pad those quite a bit to account for possibly-bad WCSs.  The above query actually returns all of the calexps in the (small) ci_hsc dataset, because there's so much padding, and what's worse, it returns some of them several times (remember that there's no guarantee about uniqueness).\n",
    "\n",
    "But, unlike Gen2, everything it returns actually does exist in the data repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've passed a single dataset type and a single collection.  You can also pass `...` for either argument to look for all dataset types and/or in all collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(butler.registry.queryDatasets(..., collections=[\"ref_cats\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.Pattern` objects (what `re.compile` returns) are also accepted for either `collections` or `datasetType`, or both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ref.datasetType for ref in butler.registry.queryDatasets(re.compile(\"deepCoadd.+image\"), collections=...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(ref.datasetType for ref in butler.registry.queryDatasets(..., collections=[re.compile(\"LSST-ImSim\\/.*\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `queryDataset` allows you to pass a [boolean expression (in mostly SQL-like syntax)](https://pipelines.lsst.io/v/weekly/modules/lsst.daf.butler/exprParser.html) that involves any dimension field (including metadata):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(butler.registry.queryDatasets(\"raw\", collections=[\"LSST-ImSim/raw/all\"], where=\"visit < 903338 AND detector IN (15..50)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data IDs and Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the biggest differences in `get` between Gen2 and Gen3 are in the data ID.  Here's the Gen3 data ID again, along with its Gen2 counterpart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataId_gen2 = {\"tract\": 3078, \"patch\": \"5,4\", \"filter\": \"z\"}\n",
    "print(f\"Gen3: {dataId}\")\n",
    "print(f\"Gen2: {dataId_gen2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two key-value pairs are the same: `tract=3078`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key-value pair has clearly has the same intent, but has both a different key: `abstract_filter=\"z\"`.  Gen3 distinguishes between \"physical\" filters, which are associated with a particular piece of glass on a particular instrument (these have names like \"HSC-R\"), and \"abstract\" filters, which are named groups of similar filters (with names like \"r\").  For some Gen2 data IDs, the value would have been different as well, because we weren't consistent about whether to use physical filter names or abstract filter names for coadds.  In Gen3, the coadd dataset types are all defined in terms of `abstract_filter`.  That's not just so we can coadd data from multiple instruments together (this is just one of several steps we'd need to enable that) - it also helps with cameras like HSC that have two versions of the same filter (i.e. \"HSC-R\" and \"HSC-R2\" are both `physical_filters`) that we want to be able to combine.\n",
    "\n",
    "_Unfortunately, it looks like we haven't consistently defined filters for LSST-ImSim, even in Gen3 - it looks like the names are just strings like \"z\", but those should probably include something that specifies the instrument._\n",
    "\n",
    "Right now, each `physical_filter` corresponds to exactly one `abstract_filter` (it's many-to-one).  We know that reality is more complex than that (many-to-many), and we expect to generalize this in the future.  We're thinking about renaming `abstract_filter` to just `filter` along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `skymap` key is a totally new one.  In Gen3, all data IDs that involve `tract` also need to involve a `skymap` key that indicates which `skymap` defines that tract.  New skymaps can be added to a `Registry` by calling [BaseSkyMap.register](https://pipelines.lsst.io/v/weekly/py-api/lsst.skymap.BaseSkyMap.html#lsst.skymap.BaseSkyMap.register) or running the [makeGen3SkyMap.py](https://github.com/lsst/pipe_tasks/blob/master/bin.src/makeGen3Skymap.py) command-line tool (which also writes a `deepCoadd_skyMap` dataset, which we usually want).  A skymap must be added to the `Registry` before we can run any `PipelineTask` or `put` any dataset that uses it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `patch` key is present in both the Gen2 and Gen3 data IDs, but with a different value.  That's because (as per [RFC-365](https://jira.lsstcorp.org/browse/RFC-365)) `patch` identifiers in Gen3 are single integers that encode both the `x` and `y` indices.  We'll show later how to convert between these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataCoordinate and Dimension instances\n",
    "\n",
    "While you can still pass simple dictionaries as arguments to `Butler` and `Registry` APIs that expect data IDs, the objects we get back from the butler are always instances of `DataCoordinate`.  In fact, the data ID associated with a `DatasetRef` is a `DataCoordinate` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.dataId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `DataCoordinate.__repr__` still prints something that looks _almost_ like a `dict` (note the lack of quotes around the keys), and unlike many `__repr__` strings, it isn't something you could execute to reconstruct the object.  That's because there's really nothing at all concise we could print to let you reconstruct it, and we decided to emphasize concise readability instead.\n",
    "\n",
    "`DataCoordinate` instances are dict-like objects, and they can be passed anywhere a dictionary-like data ID can be passed; many _internal_ `daf_butler` APIs require them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing worth noting about a `DataCoordinate` is that its keys aren't actually strings; they're instances of the `Dimension` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.dataId.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dimension` instances are comparable to the strings that identify them, but they're much more than labels.  Using dimensions to identify datasets is a core concept for the Gen3 butler, and you can find a lot more information on it in the [API documentation](https://pipelines.lsst.io/v/weekly/modules/lsst.daf.butler/dimensions.html).  We'll cover the basics here.\n",
    "\n",
    "Most - but not all - dimensions are associated with a table in the `Registry` database.  The rows in those tables contain the valid data ID values for that dimension, but they can also contain metadata fields and foreign key fields that are used to model relationships between dimensions.  The rows in the dimension tables are the same across all collections - that's important, because we want the meaning of a data ID to be consistent across (e.g.) different processing runs.  Dimensions form a sort of scaffolding or skeleton for datasets - datasets do not have relationships of their own; instead we rely on the web of dimension relationships to connect them.\n",
    "\n",
    "The full set of dimensions known to a `Registry` is stored in a class called the `DimensionUniverse`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler.registry.dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get dimension instances via dict-like indexing on this object, and we'll use that to take a closer look at the `visit` dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit = butler.registry.dimensions[\"visit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this object represents the _concept_ of a visit, not a particular visit.  The row in the database that corresponds to a single visit is represented in Python by a dynamically generated type you can access via `visit.RecordClass`.\n",
    "\n",
    "Let's start by printing the fields of the visit table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit.RecordClass.__slots__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these (`exposure_time`, `seeing`) are just metadata fields with no special structure.  We'll have to add more of these to `visit` in the future - this just isn't something we've tried to be comprehensive about yet.\n",
    "\n",
    "The `region` and `datetime*` fields are present because `visit` is `spatial` and `temporal`.  These define implied relationships to any other dimension that is spatial and/or temporal, via the overlap of their regions and/or timespans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `id` field is the _primary key_ for `visit`, which means it's what you use as the value in data ID key-value pairs.  `name` is an _alternate key_, which means it also uniquely identifies a visit, and someday we plan to make those usable as data ID values (but haven't yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(visit.primaryKey.name, visit.primaryKey.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _database_ primary key for the `visit` table isn't just `id`, though - `visit` has a _required dependency_ on the `instrument` dimension, which means it has a foreign key to the `instrument` table that is also part of its (compound) primary key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(visit.required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This required dependency means that whenever the `visit` key appears in a data ID, the `instrument` key must as well.  You've already seen another example of this: the `tract` dimension has a required dependency on the `skymap` dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(butler.registry.dimensions[\"tract\"].required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `visit` has an _implied dependency_ on the `physical_filter` dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(visit.implied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the database side, that means that the `visit` table has a `physical_filter` field that is a foreign key but _not_ a primary key.  In terms of data IDs, this means that you _don't_ need to pass a `physical_filter` key in a data ID that involves `visit`, but the `Registry` can add one for you based on what's in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanded DataCoordinates\n",
    "\n",
    "You can get that extra dimension information from the `Registry` by calling `expandDataId`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded = butler.registry.expandDataId({\"instrument\": \"LSST-ImSim\", \"visit\": 443964})\n",
    "expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you expand a data ID like this, you get everything the database knows about those dimensions.  In order to make `DataCoordinate`s that do have this extra information behave compatibly with those that don't, its behavior can be a little tricky; while you can ask it for the values of implied dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded[\"physical_filter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they don't appear in `keys()` or iteration, which are still just the required dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a dict with the full set of keys with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dict(expanded.full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `abstract_filter` is here, too, not just `physical_filter`.  That's because `physical_filter` has an implied dependency on `abstract_filter`, and those dependencies are expanded recursively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(butler.registry.dimensions[\"physical_filter\"].implied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataCoordinate.records` is a dictionary with all of the metadata of all of the dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for element, record in expanded.records.items():\n",
    "    print(element.name, record.toDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can ask a `DataCoordinate` for its `region`, which is a [lsst.sphgeom.ConvexPolygon](http://doxygen.lsst.codes/stack/doxygen/x_masterDoxyDoc/classlsst_1_1sphgeom_1_1_convex_polygon.html) if the data ID corresponds to a region on the sky, or `None` if it does not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded.region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed earlier, this region may be heavily padded to account for inaccurate initial WCSs, but they should be guaranteed to contain the true region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DimensionGraph\n",
    "\n",
    "The sometimes-complex system of relationships between dimensions makes it very useful to have a specialized container for them, and we've actually already seen this class (`DimensionGraph`) in use in a few places:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepCoaddType.dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.dataId.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DimensionUniverse` is a special subclass of `DimensionGraph` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore `DimensionGraph` further, we'll start by extracting an interesting and common set of dimensions from the universe - these are the ones used to label the `calexp` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = butler.registry.dimensions.extract([\"detector\", \"visit\"])\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note here is that the set of dimensions is automatically expanded to include all (recursive) required and implied dimensions.  The dimensions are also sorted topologically (dependents follow their dependencies), with string (lexicographical) comparisons to break ties.\n",
    "\n",
    "We can ask a `DimensionGraph` for its `required` and `implied` dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(graph.required)\n",
    "print(graph.implied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that which dimensions are `implied` depends on which dimensions are present; `physical_filter` is only `implied` here because `visit` is also in the graph.\n",
    "\n",
    "The *required* dimensions of a graph are particularly important, because those are the keys of a `DataCoordinate` that identifies the dimensions of that graph.\n",
    "\n",
    "You can also ask a `DimensionGraph` for its `temporal` and `spatial` dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(graph.temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(graph.spatial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That last answer is a bit unexpected - `visit_detector_region` isn't even one of the dimensions in the graph!  Instead, it's a table that's part of the dimensions system, without being an actual `Dimension` itself.  It can't be used as a data ID key, but it is used to provide other information about true dimensions.  In this case, that extra information is the `region` associated with the _combination_ of a `visit` and a `detector`.  A `visit` has its own region, but the system knows that the one provided by `visit_detector_region` is more specific and hence a better match for this graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkyPix Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `htm7` and `htm9` dimensions in this universe are special; they're represented in code by the `SkyPixDimension` subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htm9 = butler.registry.dimensions[\"htm9\"]\n",
    "htm9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A skypix dimension represents a particular level of a particular hierarchical pixelization of the sky, which corresponds to an instance of [lsst.sphgeom.Pixelization](http://doxygen.lsst.codes/stack/doxygen/x_masterDoxyDoc/classlsst_1_1sphgeom_1_1_pixelization.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htm9.pixelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Pixelization` instance knows how to go from integer IDs to regions on the sky and back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(htm9.pixelization.envelope(expanded.region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(htm9.pixelization.pixel(3031552))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these mappings are fully defined in code, we don't also put them in the database, so `skypix` dimensions don't have their own entries in the `Registry` database.\n",
    "\n",
    "However, we also use *one* skypix dimension, called the \"common\" skypix dimension, as a sort of spatial index that relates all other spatial dimensions in the database.  It still doesn't have its own table, but the database does have join tables that relate the common skypix dimension's IDs to the primary keys of other dimensions.  Usually this should be completely transparent.  You can get the common skypix dimension from the universe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler.registry.dimensions.commonSkyPix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `Registry.queryDimensions` to run complex queries that return data IDs.  It accepts many of the same arguments as `queryDatasets`, and also returns iterators.  The first set of arguments is the set of dimensions the returned data IDs should include.  This can be any iterable over strings or `Dimension` instances, and will be expanded to a self-consistent `DimensionGraph` automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataId in butler.registry.queryDimensions([\"visit\"], instrument=\"LSST-ImSim\", physical_filter=\"z\"):\n",
    "    print(dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that you have to include `instrument` here if you want to pass `physical_filter` as a partial data ID (the same would be true in `queryDataset`) because we convert the given data ID into an `DataCoordinate` *before* we run the query.  That can't work unless you specify the `instrument`, because the `physical_filter` dimension has a required dependency on the `instrument` dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataId in butler.registry.queryDimensions([\"visit\"], physical_filter=\"HSC-I\"):\n",
    "    print(dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You _can_ query on just `physical_filter` using a string expression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataId in butler.registry.queryDimensions([\"visit\"], where=\"physical_filter = 'HSC-I'\"):\n",
    "    print(dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but this may not be what you want.  In a larger, more realistic repository, this will actually search over all instruments, and while our _convention_ of putting the instrument name in the physical filter string would still save us from getting results from those other instruments here, it might be less efficient, and variations on this case involving other dimensions might produce undesired results.  From that perspective, the requirement that any data ID passed be complete and self-consistent is a feature, not a bug, and we relax it in the string-based query system because it's intended to be much more flexible (and hence can't be as careful)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `queryDatasets`, `queryDimensions` will automatically use spatial or temporal relationships, as well as implied dimension relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataId in butler.registry.queryDimensions([\"visit\"], dataId={\"skymap\": \"imsim_skymap\", \"tract\": 3078, \"patch\": 26, \"abstract_filter\": \"z\"}):\n",
    "    print(dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And while data IDs are not dependent on any dataset type or collection, you can query for the data IDs for which some set of datasets (all) exist in one or more collections.  For example, this query returns all detectors for which a `flat` dataset exists in the `HSC/calib` collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set(butler.registry.queryDimensions([\"detector\"], datasets=\"flat\", collections=\"LSST-ImSim/calib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well.  That worked, but it actually gave us much more than we asked for, which was data IDs with just `detector` (and `instrument`, since that's a required dependency).  This is a bug, and I created a ticket for it ([DM-22176](https://jira.lsstcorp.org/browse/DM-22176)) the first time I gave (an earlier version of this tutorial) a few months ago.  Fixing it just hasn't been a priority.\n",
    "\n",
    "Not a bad thing to close on a pretty representative example of the fact that this is all still under construction, even if it can already do a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: what about `put`?\n",
    "\n",
    "It's just like `get`, but you pass the thing you want to write as the first argument, and you can't use `parameters`.  You can use `DatasetRef`.  There is no return value.  You can find more documentation [here](https://pipelines.lsst.io/py-api/lsst.daf.butler.Butler.html#lsst.daf.butler.Butler.put).\n",
    "\n",
    "That won't work here (by design; there's a good chance you're using a shared example repo we don't want to break), because the `Butler` we constructed is read-only.  If you want a read-write `Butler`, construct the `Butler` with a `run` argument instead of a `collections` argument, or pass `writeable=True`.  A \"run\" is a special kind of collection; the [daf_butler docs](https://pipelines.lsst.io/v/weekly/modules/lsst.daf.butler/organizing.html#collections) have a more complete description of the types of collections.  The `shared/ci_hsc_output` collection we've been using is actually the name of a run-type collection, but passing it as a regular collection tells the `Butler` that we don't plan to write anything to it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
